{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising of SAR data using Deep Learning\n",
    "### Lorenz Bacca \n",
    "### Interdisciplinary Project in Data Science\n",
    "\n",
    "This notebook is used to train and further evaluate the deep learning model that tries to remove speckle from SAR data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Loading needed libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from model import Autoencoder\n",
    "from utils import SARImageDataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "import math\n",
    "from scipy.ndimage import uniform_filter\n",
    "from statistics import variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(t, p, scale = 255):\n",
    "    return np.mean(np.square(t * scale - p * scale))\n",
    "    \n",
    "def psnr(t, p, scale = 255):\n",
    "    return 20 * math.log10(scale) - 10 * math.log10(mse(t, p))\n",
    "\n",
    "def lee_filter(input_array, size=5):\n",
    "    \"\"\"\n",
    "     Python Implementation of lee filter function. Removes speckle like noise from an input image using a weighted uniform filter\n",
    "\n",
    "     Parameters\n",
    "     __________\n",
    "     input_array: str\n",
    "         numpy array of the input image\n",
    "     size: int\n",
    "         size of the filter kernel\n",
    "\n",
    "     Returns\n",
    "     _______\n",
    "     out_array: array\n",
    "         filtered output array\n",
    "     \"\"\"\n",
    "    # Calculate array mean, square mean and variance\n",
    "    img_mean = uniform_filter(input_array, (size, size))\n",
    "    img_sqr_mean = uniform_filter(input_array**2, (size, size))\n",
    "    img_variance = img_sqr_mean - img_mean**2\n",
    "\n",
    "    # Calculate the overall variance to determine the weights for the smoothing\n",
    "    overall_variance = np.var(input_array.flatten())\n",
    "\n",
    "    # remove speckle by smoothing the input. The higher the deviation from the mean, the higher the smoothing weight\n",
    "    img_weights = img_variance / (img_variance + overall_variance)\n",
    "    out_array = img_mean + img_weights * (input_array - img_mean)\n",
    "    return out_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, testloader, NUM_EPOCHS):\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    epochs = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_train_loss = []\n",
    "        running_loss = 0.0\n",
    "        for data in trainloader:\n",
    "            input_img, output_img = data\n",
    "            input_img = input_img.to('cuda:0')\n",
    "            output_img = output_img.to('cuda:0')\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(input_img)\n",
    "            loss = criterion(outputs, output_img)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            prediction = outputs.cpu().detach().numpy()[0][0]\n",
    "            target, x = data\n",
    "            \n",
    "            epoch_train_loss.append(psnr(target.numpy()[0][0], prediction))\n",
    "        \n",
    "        loss = running_loss / len(trainloader)\n",
    "        train_loss.append(np.mean(epoch_train_loss))\n",
    "        eval_loss = test(net, testloader)\n",
    "        test_loss.append(eval_loss)\n",
    "        epochs.append(epoch + 1)\n",
    "        print('Epoch {} of {}, Train Loss: {:.10f}, Test Loss: {:.3f}'.format(\n",
    "            epoch+1, NUM_EPOCHS, np.mean(epoch_train_loss), eval_loss))\n",
    "        \n",
    "    return (epochs, train_loss, test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, testloader):\n",
    "    data = SARImageDataset(transform=transform, target_transform=transform, train = False)\n",
    "    dataloader = DataLoader(data, batch_size=1, shuffle=True)\n",
    "    \n",
    "    psnr_loss = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        train_features, train_labels = next(iter(dataloader))\n",
    "        prediction = net(train_features.to('cuda:0'))\n",
    "        \n",
    "        source = train_features.numpy()[0][0]\n",
    "        target = train_labels.numpy()[0][0]\n",
    "        pred = prediction.cpu().detach().numpy()[0][0]\n",
    "        \n",
    "        psnr_loss.append(psnr(target, pred))\n",
    "    \n",
    "    return np.mean(psnr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Autoencoder()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 5e-4\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Creating train and test dataset and executing training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = SARImageDataset(transform = transform, target_transform=transform, train = True)\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_data = SARImageDataset(transform = transform, target_transform=transform, train = False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "net.to('cuda:0')\n",
    "train_loss = train(net, train_dataloader, test_dataloader, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Visual and quantitative evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "data = SARImageDataset(transform=transform, target_transform=transform, train = False)\n",
    "dataloader = DataLoader(data, batch_size=1, shuffle=True)\n",
    "\n",
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(dataloader))\n",
    "prediction = net(train_features.to('cuda:0'))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "#subplot(r,c) provide the no. of rows and columns\n",
    "f, axarr = plt.subplots(1,6, figsize=(22,4), dpi=100) \n",
    "\n",
    "source = train_features.numpy()[0][0]\n",
    "target = train_labels.numpy()[0][0]\n",
    "pred = prediction.cpu().detach().numpy()[0][0]\n",
    "pred_lee = lee_filter(source)\n",
    "pred_target = target - pred\n",
    "pred_lee_target = target - pred_lee\n",
    "\n",
    "#labels\n",
    "axarr[0].set_title('Input image')\n",
    "axarr[1].set_title('Target image')\n",
    "axarr[2].set_title('Lee filter prediction')\n",
    "axarr[3].set_title('DL model prediction')\n",
    "axarr[4].set_title('Lee filter - target')\n",
    "axarr[5].set_title('DL model - target')\n",
    "\n",
    "# use the created array to output your multiple images\n",
    "axarr[0].imshow(source, cmap=cm.gray)\n",
    "axarr[1].imshow(target, cmap=cm.gray)\n",
    "axarr[2].imshow(pred_lee, cmap=cm.gray)\n",
    "axarr[3].imshow(pred, cmap=cm.gray)\n",
    "axarr[4].imshow(pred_lee, cmap=cm.jet)\n",
    "axarr[5].imshow(pred_target, cmap=cm.jet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss_model = []\n",
    "psnr_loss_model = []\n",
    "\n",
    "mse_loss_lee = []\n",
    "psnr_loss_lee = []\n",
    "\n",
    "for data in dataloader:\n",
    "    train_features, train_labels = next(iter(dataloader))\n",
    "    prediction = net(train_features.to('cuda:0'))\n",
    "    \n",
    "    source = train_features.numpy()[0][0]\n",
    "    target = train_labels.numpy()[0][0]\n",
    "    pred = prediction.cpu().detach().numpy()[0][0]\n",
    "    \n",
    "    pred_lee = lee_filter(source)\n",
    "    \n",
    "    psnr_loss_model.append(psnr(target, pred))\n",
    "    mse_loss_model.append(mse(target, pred))\n",
    "    \n",
    "    psnr_loss_lee.append(psnr(target, pred_lee))\n",
    "    mse_loss_lee.append(mse(target, pred_lee))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.median(psnr_loss_model))\n",
    "print(np.median(psnr_loss_lee))\n",
    "\n",
    "print(np.median(mse_loss_model))\n",
    "print(np.median(mse_loss_lee))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Saving and loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(net.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "model.cuda()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
